{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87760df-c920-4d22-a0f1-1cf9eb6f386f",
   "metadata": {},
   "source": [
    "# RICC-HAT: An Interactive Tutorial on Self-Improving Hardware\n",
    "\n",
    "Welcome! This notebook demonstrates the full power of the Hardware-Aware Training (HAT) paradigm, as presented in our paper, \"Resilience that Learns.\"\n",
    "\n",
    "We will show not only how HAT creates robust agents, but also how it enables a foundational principle of **self-improving systems**: the ability to learn and adapt to physical changes *after* deployment, such as component aging.\n",
    "\n",
    "**This tutorial is in two parts:**\n",
    "1.  **Part 1: Pre-Silicon Training:** We will train two agents—one on \"ideal\" hardware and one on \"realistic\" HAT hardware—to show why HAT is necessary for initial robustness.\n",
    "2.  **Part 2: Post-Deployment Adaptation:** We will simulate hardware aging and demonstrate how the HAT-trained agent can be rapidly fine-tuned to recover its performance, effectively \"learning\" to compensate for its own degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210be7d-052a-4f6e-a1f3-7496956f567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.hat_framework import stochastic_forward_pass\n",
    "\n",
    "# NEW: We define a modified HAT function to simulate \"aged\" hardware.\n",
    "# Let's assume aging introduces a systematic offset and increases noise.\n",
    "def stochastic_forward_pass_aged(linear_layer, x):\n",
    "    # Call the original function to get the baseline noisy output\n",
    "    base_output = stochastic_forward_pass(linear_layer, x)\n",
    "    \n",
    "    # Introduce an aging-induced systematic offset and more noise\n",
    "    aging_offset = 0.05\n",
    "    aging_noise_factor = 1.5\n",
    "    \n",
    "    aged_output = base_output * aging_noise_factor + aging_offset\n",
    "    return aged_output\n",
    "\n",
    "# Update the Actor network to handle the 'aged' state\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(1, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, hardware_mode='ideal'):\n",
    "        # hardware_mode can be 'ideal', 'hat', or 'hat_aged'\n",
    "        if hardware_mode == 'ideal':\n",
    "            x = torch.relu(self.layer1(x))\n",
    "            x = torch.relu(self.layer2(x))\n",
    "        elif hardware_mode == 'hat':\n",
    "            x = torch.relu(stochastic_forward_pass(self.layer1, x))\n",
    "            x = torch.relu(stochastic_forward_pass(self.layer2, x))\n",
    "        elif hardware_mode == 'hat_aged':\n",
    "            x = torch.relu(stochastic_forward_pass_aged(self.layer1, x))\n",
    "            x = torch.relu(stochastic_forward_pass_aged(self.layer2, x))\n",
    "        \n",
    "        x = torch.tanh(self.layer3(x))\n",
    "        return x\n",
    "        \n",
    "# (Other setup like data and training function remains the same...)\n",
    "X_train = torch.linspace(-np.pi, np.pi, 512).unsqueeze(1)\n",
    "y_train = torch.sin(X_train)\n",
    "\n",
    "def train_model(model, optimizer, hardware_mode, steps=1500):\n",
    "    for step in range(steps):\n",
    "        predictions = model(X_train, hardware_mode=hardware_mode)\n",
    "        loss = nn.functional.mse_loss(predictions, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7c6c4-be5e-4f62-89ea-526eb2a99056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 1: Initial training \"at the factory\" ---\n",
    "print(\"--- Training IDEAL Model (on Perfect Hardware) ---\")\n",
    "ideal_model = Actor()\n",
    "ideal_optimizer = torch.optim.Adam(ideal_model.parameters(), lr=0.001)\n",
    "ideal_model = train_model(ideal_model, ideal_optimizer, hardware_mode='ideal')\n",
    "\n",
    "print(\"\\n--- Training RICC-HAT Model (on Realistic Hardware) ---\")\n",
    "hat_model_initial = Actor()\n",
    "hat_optimizer = torch.optim.Adam(hat_model_initial.parameters(), lr=0.001)\n",
    "hat_model_initial = train_model(hat_model_initial, hat_optimizer, hardware_mode='hat')\n",
    "\n",
    "print(\"\\nInitial training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d8a7a-7bd9-44fb-95fa-6a0daaa34e5a",
   "metadata": {},
   "source": [
    "### Part 2: Simulating \"Aging\" and Fine-Tuning\n",
    "\n",
    "Now, let's simulate what happens after the chip has been in the field for a long time. The physical hardware has degraded (`hat_aged`). How will our factory-trained models perform?\n",
    "``````python\n",
    "# --- Evaluate performance on the \"aged\" hardware ---\n",
    "with torch.no_grad():\n",
    "    ideal_pred_on_aged = ideal_model(X_train, hardware_mode='hat_aged')\n",
    "    hat_pred_on_aged_initial = hat_model_initial(X_train, hardware_mode='hat_aged')\n",
    "\n",
    "# Calculate the error (loss)\n",
    "loss_ideal_on_aged = nn.functional.mse_loss(ideal_pred_on_aged, y_train)\n",
    "loss_hat_on_aged_initial = nn.functional.mse_loss(hat_pred_on_aged_initial, y_train)\n",
    "\n",
    "print(f\"Error of Ideal Model on Aged Hardware: {loss_ideal_on_aged:.4f}\")\n",
    "print(f\"Error of Initial HAT Model on Aged Hardware: {loss_hat_on_aged_initial:.4f} (Performance has degraded!)\")\n",
    "\n",
    "# Plot the degradation\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_train, y_train, 'k--', label='Target Function', alpha=0.5)\n",
    "plt.plot(X_train, hat_pred_on_aged_initial, 'r-', label=f'Initial HAT Model on Aged HW (Error: {loss_hat_on_aged_initial:.4f})')\n",
    "plt.title('Performance Degradation due to Hardware Aging', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c3b65-0e5b-4cf7-9d38-a04ebe311a8b",
   "metadata": {},
   "source": [
    "As we can see, the performance of our HAT-trained agent has degraded due to the physical changes in the hardware.\n",
    "\n",
    "Now, we will demonstrate the **self-improving** capability. We will perform a short \"fine-tuning\" (re-training) session, allowing the agent to adapt to its new physical reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae043d8-0a5e-4386-bbea-d636da91bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fine-tune the HAT model on the aged hardware ---\n",
    "print(\"\\n--- Fine-tuning the HAT model on its new reality... ---\")\n",
    "# We start from the already-trained model (hat_model_initial)\n",
    "hat_model_finetuned = hat_model_initial\n",
    "finetune_optimizer = torch.optim.Adam(hat_model_finetuned.parameters(), lr=0.0001) # Use a smaller learning rate\n",
    "hat_model_finetuned = train_model(hat_model_finetuned, finetune_optimizer, hardware_mode='hat_aged', steps=500) # Only a few steps needed\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32723b61-d2cb-4f88-83eb-d226c695ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final evaluation and comparison ---\n",
    "with torch.no_grad():\n",
    "    hat_pred_on_aged_finetuned = hat_model_finetuned(X_train, hardware_mode='hat_aged')\n",
    "    \n",
    "loss_hat_on_aged_finetuned = nn.functional.mse_loss(hat_pred_on_aged_finetuned, y_train)\n",
    "print(f\"Error of Fine-Tuned HAT Model on Aged Hardware: {loss_hat_on_aged_finetuned:.4f} (Performance Recovered!)\")\n",
    "\n",
    "# Plot the final comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_train, y_train, 'k--', label='Target Function', linewidth=3, alpha=0.5)\n",
    "plt.plot(X_train, hat_pred_on_aged_initial, 'r-', label=f'Before Fine-Tuning (Error: {loss_hat_on_aged_initial:.4f})', alpha=0.7)\n",
    "plt.plot(X_train, hat_pred_on_aged_finetuned, 'g-', label=f'After Fine-Tuning (Error: {loss_hat_on_aged_finetuned:.4f})', linewidth=2.5)\n",
    "plt.title('HAT Agent Performance: Before and After Self-Adaptation', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b743d8-003f-4c27-973f-05b784f8bb8f",
   "metadata": {},
   "source": [
    "### Final Analysis\n",
    "\n",
    "This tutorial demonstrated the full, two-phase power of the Hardware-Aware Training framework:\n",
    "1.  **Initial Robustness:** We created a model that was robust \"out-of-the-box.\"\n",
    "2.  **Long-Term Adaptability:** Crucially, we showed that when the physical world changed (due to aging), the agent could **perceive this change and adapt**. By fine-tuning its policy on its new physical reality, it successfully recovered its high performance.\n",
    "\n",
    "This is the essence of a **self-improving system**, and it validates the core premise of our work: creating hardware that doesn't just endure, but *learns*.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdd8ae-d251-4b48-8e4f-e7e91360a3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
