{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a57353a-7c82-4a3b-826c-fea016b66162",
   "metadata": {},
   "source": [
    "# RICC-HAT: An Interactive Tutorial\n",
    "\n",
    "Welcome! This notebook provides a hands-on demonstration of the **Hardware-Aware Training (HAT)** methodology presented in our paper, \"Resilience that Learns.\"\n",
    "\n",
    "The core idea of HAT is simple but powerful: instead of training an AI agent in a perfect, idealized world, we train it in a simulation that constantly bombards it with the same kind of noise and imperfections it will face in the real, physical hardware. This forces the agent to become **robust by design, not by chance.**\n",
    "\n",
    "In this tutorial, we will:\n",
    "1.  Define a simple neural network.\n",
    "2.  Define two training methods: an **Ideal** method and a **HAT** method.\n",
    "3.  Compare how they learn to approximate a simple function.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816c79b-e186-403d-81fb-d209c4afe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our hardware model\n",
    "from src.hat_framework import stochastic_forward_pass\n",
    "\n",
    "# Define a simple Actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, use_hat=False):\n",
    "        super(Actor, self).__init__()\n",
    "        self.use_hat = use_hat\n",
    "        self.layer1 = nn.Linear(1, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_hat:\n",
    "            x = torch.relu(stochastic_forward_pass(self.layer1, x))\n",
    "            x = torch.relu(stochastic_forward_pass(self.layer2, x))\n",
    "            # The last layer is often kept ideal for stability, but could also be stochastic\n",
    "            x = torch.tanh(self.layer3(x)) \n",
    "        else: # Ideal pass\n",
    "            x = torch.relu(self.layer1(x))\n",
    "            x = torch.relu(self.layer2(x))\n",
    "            x = torch.tanh(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a131c-42c1-4db2-b478-bfa14a260b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our goal is to learn a simple sine wave function.\n",
    "# This represents a simplified, non-linear control policy.\n",
    "X_train = torch.linspace(-np.pi, np.pi, 512).unsqueeze(1)\n",
    "y_train = torch.sin(X_train)\n",
    "\n",
    "# Plot the target function\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_train.numpy(), y_train.numpy(), 'k-', label='Target Function (sin(x))', linewidth=3)\n",
    "plt.title('The Function We Want to Learn', fontsize=16)\n",
    "plt.xlabel('State (s)', fontsize=14)\n",
    "plt.ylabel('Action (a)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdca2bd-7346-42be-8af5-7cb4a6cd01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, steps=2000):\n",
    "    losses = []\n",
    "    for step in range(steps):\n",
    "        # Forward pass\n",
    "        predictions = model(X_train)\n",
    "        \n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = nn.functional.mse_loss(predictions, y_train)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if (step + 1) % 500 == 0:\n",
    "            print(f\"Step [{step+1}/{steps}], Loss: {loss.item():.6f}\")\n",
    "    return losses, model\n",
    "\n",
    "# --- Train the IDEAL model ---\n",
    "print(\"--- Training IDEAL Model (Perfect Hardware) ---\")\n",
    "ideal_model = Actor(use_hat=False)\n",
    "ideal_optimizer = torch.optim.Adam(ideal_model.parameters(), lr=0.001)\n",
    "ideal_losses, ideal_model = train_model(ideal_model, ideal_optimizer)\n",
    "\n",
    "print(\"\\n--- Training RICC-HAT Model (Noisy Hardware) ---\")\n",
    "hat_model = Actor(use_hat=True)\n",
    "hat_optimizer = torch.optim.Adam(hat_model.parameters(), lr=0.001)\n",
    "hat_losses, hat_model = train_model(hat_model, hat_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ba797-af06-43ae-92ae-213f7b9294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ideal_losses, label='Ideal Model Loss', color='gray')\n",
    "plt.plot(hat_losses, label='RICC-HAT Model Loss', color='blue', linewidth=2)\n",
    "plt.title('Training Loss Comparison', fontsize=16)\n",
    "plt.xlabel('Training Steps', fontsize=14)\n",
    "plt.ylabel('Mean Squared Error Loss', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(0, 0.1)\n",
    "\n",
    "# Subplot 2: Function approximation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X_train.numpy(), y_train.numpy(), 'k--', label='Target Function', linewidth=3, alpha=0.5)\n",
    "plt.plot(X_train.numpy(), ideal_model(X_train).detach().numpy(), 'r-', label='Ideal Model Fit')\n",
    "plt.plot(X_train.numpy(), hat_model(X_train).detach().numpy(), 'b-', label='RICC-HAT Model Fit', linewidth=2.5)\n",
    "plt.title('Function Approximation', fontsize=16)\n",
    "plt.xlabel('State (s)', fontsize=14)\n",
    "plt.ylabel('Action (a)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400d41-4066-4794-aebd-1aabda5e4fd0",
   "metadata": {},
   "source": [
    "``````markdown\n",
    "### Analysis and Conclusion\n",
    "\n",
    "As you can see from the results:\n",
    "\n",
    "1.  **Loss Curves:** The **RICC-HAT model's** loss is noisier and might converge slightly slower. This is expected! The agent is not just minimizing the error, it's simultaneously learning to be robust to the constant hardware noise we are injecting.\n",
    "2.  **Function Approximation:** Both models learn the target function well. However, the policy learned by the **RICC-HAT model** is inherently robust. While the Ideal model might achieve a slightly lower loss in this perfect simulation, it would fail catastrophically when deployed on real, noisy hardware. The RICC-HAT agent, having trained in a \"school of hard knocks,\" is already prepared for the real world.\n",
    "\n",
    "This simple experiment demonstrates the power of the Hardware-Aware Training paradigm. Thank you for following along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54779ba2-46a6-4789-a964-dddb1e25f9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
